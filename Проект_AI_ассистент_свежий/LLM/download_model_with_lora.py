from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig
import torch
import os

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
BASE_MODEL = "Qwen/Qwen2-1.5B-Instruct"
LORA_MODEL = "SpaWn03/fixed-qwen2-1.5b-instruct-business-assistant"
LOCAL_PATH = "./models/qwen2-1.5b-lora"

# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –µ—Å–ª–∏ –Ω–µ—Ç
os.makedirs(LOCAL_PATH, exist_ok=True)

print("üì• 1/4 –°–∫–∞—á–∏–≤–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å...")
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.float32,
    low_cpu_mem_usage=True
)

print("üì• 2/4 –°–∫–∞—á–∏–≤–∞–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä...")
model = PeftModel.from_pretrained(base_model, LORA_MODEL)

print("üîó 3/4 –û–±—ä–µ–¥–∏–Ω—è–µ–º –º–æ–¥–µ–ª—å —Å LoRA...")
model = model.merge_and_unload()  # –í–∞–∂–Ω–æ: –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∞–¥–∞–ø—Ç–µ—Ä —Å –º–æ–¥–µ–ª—å—é

print(f"üíæ 4/4 –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –≤ {LOCAL_PATH}...")
tokenizer.save_pretrained(LOCAL_PATH)
model.save_pretrained(LOCAL_PATH)

print("‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å LoRA —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ!")