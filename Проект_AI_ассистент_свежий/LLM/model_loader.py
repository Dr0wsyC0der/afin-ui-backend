from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch
import logging
import os

logging.basicConfig(level=logging.INFO)

# –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å —Å Hugging Face Hub
MODEL_BASE = "Qwen/Qwen2-1.5B-Instruct"
# –ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –¥–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å LoRA, —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–æ–∫—É:
# MODEL_LORA = "SpaWn03/fixed-qwen2-1.5b-instruct-business-assistant"

# –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è —É–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
HF_MODEL_NAME = os.getenv("HF_MODEL_NAME", MODEL_BASE)
HF_TOKEN = os.getenv("HF_TOKEN", None)  # –î–ª—è –ø—Ä–∏–≤–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

print(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å Hugging Face: {HF_MODEL_NAME}...")
tokenizer = AutoTokenizer.from_pretrained(
    HF_MODEL_NAME,
    token=HF_TOKEN
)

model_llm = AutoModelForCausalLM.from_pretrained(
    HF_MODEL_NAME,
    torch_dtype=torch.bfloat16,
    device_map=None,
    low_cpu_mem_usage=True,
    token=HF_TOKEN
)

# –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ LoRA, —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ:
# if MODEL_LORA:
#     print(f"üîÑ –ü–æ–¥–∫–ª—é—á–∞–µ–º LoRA: {MODEL_LORA}...")
#     model_llm = PeftModel.from_pretrained(model_llm, MODEL_LORA)

model_llm.eval()
model_llm = model_llm.to("cpu")
print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞!")

__all__ = ['model_llm', 'tokenizer']